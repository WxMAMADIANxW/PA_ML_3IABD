{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dadac0d",
   "metadata": {},
   "source": [
    "Recuperation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de79757f",
   "metadata": {},
   "source": [
    "Processing des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8b088c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "637764c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"./dataset\"\n",
    "os.chdir(dataset_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439880a4",
   "metadata": {},
   "source": [
    "Stockage des images dans X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5974e392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(path,img_size):\n",
    "    list_X,list_Y = [],[]\n",
    "    for filename in tqdm(os.listdir(path)):\n",
    "        img = ImageOps.grayscale(Image.open(path+\"/\"+filename).resize(img_size))\n",
    "        array = np.asarray(img).flatten().tolist()\n",
    "        \n",
    "        list_X.append(array)\n",
    "    \n",
    "        if \"Dataset_Mangas\" in path:\n",
    "            class_array = 0\n",
    "        elif \"Dataset_Romans\" in path:\n",
    "            class_array = 1\n",
    "        elif \"Dataset_Comics\" in path:\n",
    "            class_array = 2\n",
    "        \n",
    "        list_Y.append(class_array)\n",
    "        \n",
    "      \n",
    "    X ,Y = np.array(list_X), np.array(list_Y)\n",
    "    return X,Y.reshape((Y.shape[0],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "128bd1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2034/2034 [01:37<00:00, 20.90it/s]\n"
     ]
    }
   ],
   "source": [
    "X_mangas,Y_mangas = preprocess_dataset(\"C:/Users/Mamadian/Documents/ESGI/MachineLearning/PA_ML_3IABD/dataset/Dataset_Mangas/\",(56,56))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d6521206",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2089/2089 [03:58<00:00,  8.76it/s]\n"
     ]
    }
   ],
   "source": [
    "X_comics,Y_comics = preprocess_dataset(\"C:/Users/Mamadian/Documents/ESGI/MachineLearning/PA_ML_3IABD/dataset/Dataset_Comics/\",(56,56))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "463c90ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2571/2571 [02:49<00:00, 15.15it/s]\n"
     ]
    }
   ],
   "source": [
    "X_romans,Y_romans = preprocess_dataset(\"C:/Users/Mamadian/Documents/ESGI/MachineLearning/PA_ML_3IABD/dataset/Dataset_Romans/\",(56,56))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "599198c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "007ec3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mangas_train, X_mangas_test, y_mangas_train, y_mangas_test = train_test_split(X_mangas, Y_mangas, test_size=0.33, random_state=42)\n",
    "X_romans_train, X_romans_test, y_romans_train, y_romans_test = train_test_split(X_romans, Y_romans, test_size=0.33, random_state=42)\n",
    "X_comics_train, X_comics_test, y_comics_train, y_comics_test = train_test_split(X_comics, Y_comics, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8b9f1680",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate((X_mangas_train, X_romans_train, X_comics_train), axis=0)\n",
    "y_train = np.concatenate((y_mangas_train, y_romans_train, y_comics_train), axis=0)\n",
    "\n",
    "X_test = np.concatenate((X_mangas_test, X_romans_test, X_comics_test), axis=0)\n",
    "y_test = np.concatenate((y_mangas_test, y_romans_test, y_comics_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "4164a494",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1250, alpha=1e-4,\n",
    "                    solver='sgd', verbose=10, random_state=1,\n",
    "                    learning_rate_init=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "12be3742",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255.\n",
    "X_test = X_test /255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "40b283f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.09061274\n",
      "Iteration 2, loss = 1.07651929\n",
      "Iteration 3, loss = 1.05115123\n",
      "Iteration 4, loss = 0.99609874\n",
      "Iteration 5, loss = 0.89484813\n",
      "Iteration 6, loss = 0.75590323\n",
      "Iteration 7, loss = 0.62288203\n",
      "Iteration 8, loss = 0.52384470\n",
      "Iteration 9, loss = 0.45156691\n",
      "Iteration 10, loss = 0.40251246\n",
      "Iteration 11, loss = 0.35847318\n",
      "Iteration 12, loss = 0.32508997\n",
      "Iteration 13, loss = 0.29878600\n",
      "Iteration 14, loss = 0.27798703\n",
      "Iteration 15, loss = 0.25978721\n",
      "Iteration 16, loss = 0.24485993\n",
      "Iteration 17, loss = 0.23098823\n",
      "Iteration 18, loss = 0.22387613\n",
      "Iteration 19, loss = 0.21357022\n",
      "Iteration 20, loss = 0.21039718\n",
      "Iteration 21, loss = 0.19784223\n",
      "Iteration 22, loss = 0.19390827\n",
      "Iteration 23, loss = 0.18706851\n",
      "Iteration 24, loss = 0.18332912\n",
      "Iteration 25, loss = 0.17844474\n",
      "Iteration 26, loss = 0.17395969\n",
      "Iteration 27, loss = 0.16925205\n",
      "Iteration 28, loss = 0.16491643\n",
      "Iteration 29, loss = 0.16525358\n",
      "Iteration 30, loss = 0.16107190\n",
      "Iteration 31, loss = 0.15618939\n",
      "Iteration 32, loss = 0.15638516\n",
      "Iteration 33, loss = 0.15168781\n",
      "Iteration 34, loss = 0.14879804\n",
      "Iteration 35, loss = 0.14590558\n",
      "Iteration 36, loss = 0.14440750\n",
      "Iteration 37, loss = 0.14374281\n",
      "Iteration 38, loss = 0.13974169\n",
      "Iteration 39, loss = 0.13761006\n",
      "Iteration 40, loss = 0.13548134\n",
      "Iteration 41, loss = 0.13327560\n",
      "Iteration 42, loss = 0.13056695\n",
      "Iteration 43, loss = 0.13056533\n",
      "Iteration 44, loss = 0.12856868\n",
      "Iteration 45, loss = 0.12853673\n",
      "Iteration 46, loss = 0.12421006\n",
      "Iteration 47, loss = 0.12102399\n",
      "Iteration 48, loss = 0.12136740\n",
      "Iteration 49, loss = 0.11973282\n",
      "Iteration 50, loss = 0.11671367\n",
      "Iteration 51, loss = 0.11761585\n",
      "Iteration 52, loss = 0.11534749\n",
      "Iteration 53, loss = 0.11464747\n",
      "Iteration 54, loss = 0.11127220\n",
      "Iteration 55, loss = 0.10972046\n",
      "Iteration 56, loss = 0.10789967\n",
      "Iteration 57, loss = 0.10663867\n",
      "Iteration 58, loss = 0.10454793\n",
      "Iteration 59, loss = 0.10327603\n",
      "Iteration 60, loss = 0.10064428\n",
      "Iteration 61, loss = 0.10044367\n",
      "Iteration 62, loss = 0.09756386\n",
      "Iteration 63, loss = 0.09873375\n",
      "Iteration 64, loss = 0.09588636\n",
      "Iteration 65, loss = 0.09495754\n",
      "Iteration 66, loss = 0.09354917\n",
      "Iteration 67, loss = 0.09176084\n",
      "Iteration 68, loss = 0.09044674\n",
      "Iteration 69, loss = 0.09200126\n",
      "Iteration 70, loss = 0.08866235\n",
      "Iteration 71, loss = 0.08756912\n",
      "Iteration 72, loss = 0.08690762\n",
      "Iteration 73, loss = 0.08451008\n",
      "Iteration 74, loss = 0.08362345\n",
      "Iteration 75, loss = 0.08405782\n",
      "Iteration 76, loss = 0.08194450\n",
      "Iteration 77, loss = 0.08040583\n",
      "Iteration 78, loss = 0.07911860\n",
      "Iteration 79, loss = 0.07968830\n",
      "Iteration 80, loss = 0.07646867\n",
      "Iteration 81, loss = 0.07545497\n",
      "Iteration 82, loss = 0.07681904\n",
      "Iteration 83, loss = 0.07469947\n",
      "Iteration 84, loss = 0.07384303\n",
      "Iteration 85, loss = 0.07409195\n",
      "Iteration 86, loss = 0.07179173\n",
      "Iteration 87, loss = 0.07166222\n",
      "Iteration 88, loss = 0.07292802\n",
      "Iteration 89, loss = 0.07106222\n",
      "Iteration 90, loss = 0.06874304\n",
      "Iteration 91, loss = 0.06874193\n",
      "Iteration 92, loss = 0.06780809\n",
      "Iteration 93, loss = 0.06727935\n",
      "Iteration 94, loss = 0.06578394\n",
      "Iteration 95, loss = 0.06742682\n",
      "Iteration 96, loss = 0.06657754\n",
      "Iteration 97, loss = 0.06508642\n",
      "Iteration 98, loss = 0.06598642\n",
      "Iteration 99, loss = 0.06378930\n",
      "Iteration 100, loss = 0.06491942\n",
      "Iteration 101, loss = 0.06410366\n",
      "Iteration 102, loss = 0.06155165\n",
      "Iteration 103, loss = 0.06113336\n",
      "Iteration 104, loss = 0.06280002\n",
      "Iteration 105, loss = 0.06042271\n",
      "Iteration 106, loss = 0.06245179\n",
      "Iteration 107, loss = 0.06093985\n",
      "Iteration 108, loss = 0.05907014\n",
      "Iteration 109, loss = 0.06036499\n",
      "Iteration 110, loss = 0.05848141\n",
      "Iteration 111, loss = 0.05885247\n",
      "Iteration 112, loss = 0.05874876\n",
      "Iteration 113, loss = 0.05917390\n",
      "Iteration 114, loss = 0.05955634\n",
      "Iteration 115, loss = 0.05748964\n",
      "Iteration 116, loss = 0.05658966\n",
      "Iteration 117, loss = 0.05728382\n",
      "Iteration 118, loss = 0.05475360\n",
      "Iteration 119, loss = 0.05538146\n",
      "Iteration 120, loss = 0.05605675\n",
      "Iteration 121, loss = 0.05577139\n",
      "Iteration 122, loss = 0.05392363\n",
      "Iteration 123, loss = 0.05388430\n",
      "Iteration 124, loss = 0.05379163\n",
      "Iteration 125, loss = 0.05500972\n",
      "Iteration 126, loss = 0.05246639\n",
      "Iteration 127, loss = 0.05307819\n",
      "Iteration 128, loss = 0.05273139\n",
      "Iteration 129, loss = 0.05322944\n",
      "Iteration 130, loss = 0.05408206\n",
      "Iteration 131, loss = 0.05288526\n",
      "Iteration 132, loss = 0.05250795\n",
      "Iteration 133, loss = 0.05173387\n",
      "Iteration 134, loss = 0.05174660\n",
      "Iteration 135, loss = 0.04986818\n",
      "Iteration 136, loss = 0.05243662\n",
      "Iteration 137, loss = 0.05099892\n",
      "Iteration 138, loss = 0.05094226\n",
      "Iteration 139, loss = 0.05141267\n",
      "Iteration 140, loss = 0.04909959\n",
      "Iteration 141, loss = 0.04996279\n",
      "Iteration 142, loss = 0.04964919\n",
      "Iteration 143, loss = 0.05109652\n",
      "Iteration 144, loss = 0.04876563\n",
      "Iteration 145, loss = 0.04815178\n",
      "Iteration 146, loss = 0.04768278\n",
      "Iteration 147, loss = 0.04769714\n",
      "Iteration 148, loss = 0.04704593\n",
      "Iteration 149, loss = 0.04771143\n",
      "Iteration 150, loss = 0.04813440\n",
      "Iteration 151, loss = 0.04732133\n",
      "Iteration 152, loss = 0.04730450\n",
      "Iteration 153, loss = 0.04863375\n",
      "Iteration 154, loss = 0.04629156\n",
      "Iteration 155, loss = 0.04666031\n",
      "Iteration 156, loss = 0.04566687\n",
      "Iteration 157, loss = 0.04650076\n",
      "Iteration 158, loss = 0.04637935\n",
      "Iteration 159, loss = 0.04587551\n",
      "Iteration 160, loss = 0.04602140\n",
      "Iteration 161, loss = 0.04550864\n",
      "Iteration 162, loss = 0.04558461\n",
      "Iteration 163, loss = 0.04425634\n",
      "Iteration 164, loss = 0.04600307\n",
      "Iteration 165, loss = 0.04406387\n",
      "Iteration 166, loss = 0.04650160\n",
      "Iteration 167, loss = 0.04423009\n",
      "Iteration 168, loss = 0.04383069\n",
      "Iteration 169, loss = 0.04412163\n",
      "Iteration 170, loss = 0.04470428\n",
      "Iteration 171, loss = 0.04403385\n",
      "Iteration 172, loss = 0.04606395\n",
      "Iteration 173, loss = 0.04326954\n",
      "Iteration 174, loss = 0.04310748\n",
      "Iteration 175, loss = 0.04431933\n",
      "Iteration 176, loss = 0.04264453\n",
      "Iteration 177, loss = 0.04285120\n",
      "Iteration 178, loss = 0.04479646\n",
      "Iteration 179, loss = 0.04230216\n",
      "Iteration 180, loss = 0.04329502\n",
      "Iteration 181, loss = 0.04155029\n",
      "Iteration 182, loss = 0.04386326\n",
      "Iteration 183, loss = 0.04253455\n",
      "Iteration 184, loss = 0.04165180\n",
      "Iteration 185, loss = 0.04208433\n",
      "Iteration 186, loss = 0.04273051\n",
      "Iteration 187, loss = 0.04287206\n",
      "Iteration 188, loss = 0.04055790\n",
      "Iteration 189, loss = 0.04234989\n",
      "Iteration 190, loss = 0.04086102\n",
      "Iteration 191, loss = 0.04260940\n",
      "Iteration 192, loss = 0.04182252\n",
      "Iteration 193, loss = 0.04177764\n",
      "Iteration 194, loss = 0.04093272\n",
      "Iteration 195, loss = 0.04213129\n",
      "Iteration 196, loss = 0.03977545\n",
      "Iteration 197, loss = 0.04079227\n",
      "Iteration 198, loss = 0.03921144\n",
      "Iteration 199, loss = 0.04052177\n",
      "Iteration 200, loss = 0.03966578\n",
      "Iteration 201, loss = 0.03994496\n",
      "Iteration 202, loss = 0.03967972\n",
      "Iteration 203, loss = 0.03917837\n",
      "Iteration 204, loss = 0.03940819\n",
      "Iteration 205, loss = 0.03867193\n",
      "Iteration 206, loss = 0.04171648\n",
      "Iteration 207, loss = 0.04023754\n",
      "Iteration 208, loss = 0.03898656\n",
      "Iteration 209, loss = 0.04107059\n",
      "Iteration 210, loss = 0.03983874\n",
      "Iteration 211, loss = 0.03978674\n",
      "Iteration 212, loss = 0.03988115\n",
      "Iteration 213, loss = 0.03935430\n",
      "Iteration 214, loss = 0.03772607\n",
      "Iteration 215, loss = 0.03772650\n",
      "Iteration 216, loss = 0.03757430\n",
      "Iteration 217, loss = 0.03876060\n",
      "Iteration 218, loss = 0.03884830\n",
      "Iteration 219, loss = 0.03936290\n",
      "Iteration 220, loss = 0.03736883\n",
      "Iteration 221, loss = 0.03743405\n",
      "Iteration 222, loss = 0.03881554\n",
      "Iteration 223, loss = 0.03831726\n",
      "Iteration 224, loss = 0.03779077\n",
      "Iteration 225, loss = 0.03719897\n",
      "Iteration 226, loss = 0.03827317\n",
      "Iteration 227, loss = 0.03764558\n",
      "Iteration 228, loss = 0.03706196\n",
      "Iteration 229, loss = 0.03865101\n",
      "Iteration 230, loss = 0.03924044\n",
      "Iteration 231, loss = 0.03696740\n",
      "Iteration 232, loss = 0.03759089\n",
      "Iteration 233, loss = 0.03691219\n",
      "Iteration 234, loss = 0.03587170\n",
      "Iteration 235, loss = 0.03664392\n",
      "Iteration 236, loss = 0.03782292\n",
      "Iteration 237, loss = 0.03621630\n",
      "Iteration 238, loss = 0.03767581\n",
      "Iteration 239, loss = 0.03551618\n",
      "Iteration 240, loss = 0.03633107\n",
      "Iteration 241, loss = 0.03729943\n",
      "Iteration 242, loss = 0.03590152\n",
      "Iteration 243, loss = 0.03430524\n",
      "Iteration 244, loss = 0.03631165\n",
      "Iteration 245, loss = 0.03619609\n",
      "Iteration 246, loss = 0.03551867\n",
      "Iteration 247, loss = 0.03508197\n",
      "Iteration 248, loss = 0.03479365\n",
      "Iteration 249, loss = 0.03414794\n",
      "Iteration 250, loss = 0.03564773\n",
      "Iteration 251, loss = 0.03569425\n",
      "Iteration 252, loss = 0.03455680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 253, loss = 0.03532070\n",
      "Iteration 254, loss = 0.03618118\n",
      "Iteration 255, loss = 0.03456302\n",
      "Iteration 256, loss = 0.03454324\n",
      "Iteration 257, loss = 0.03473411\n",
      "Iteration 258, loss = 0.03348647\n",
      "Iteration 259, loss = 0.03366805\n",
      "Iteration 260, loss = 0.03444210\n",
      "Iteration 261, loss = 0.03353848\n",
      "Iteration 262, loss = 0.03420598\n",
      "Iteration 263, loss = 0.03367180\n",
      "Iteration 264, loss = 0.03326561\n",
      "Iteration 265, loss = 0.03414638\n",
      "Iteration 266, loss = 0.03238749\n",
      "Iteration 267, loss = 0.03302128\n",
      "Iteration 268, loss = 0.03510612\n",
      "Iteration 269, loss = 0.03407917\n",
      "Iteration 270, loss = 0.03303733\n",
      "Iteration 271, loss = 0.03272615\n",
      "Iteration 272, loss = 0.03266837\n",
      "Iteration 273, loss = 0.03330256\n",
      "Iteration 274, loss = 0.03272555\n",
      "Iteration 275, loss = 0.03394589\n",
      "Iteration 276, loss = 0.03316979\n",
      "Iteration 277, loss = 0.03477289\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(learning_rate_init=0.1, max_iter=1250, random_state=1,\n",
       "              solver='sgd', verbose=10)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "220aa3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.989516\n",
      "Test set score: 0.985979\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set score: %f\" % mlp.score(X_train, y_train))\n",
    "print(\"Test set score: %f\" % mlp.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1063b3d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[10].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "bf687d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.predict(X_test[1000].reshape((1,3136)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "4b26855c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[1000]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
